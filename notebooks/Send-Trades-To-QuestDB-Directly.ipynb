{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0868bc-01bb-4296-8206-bad1c9b9da83",
   "metadata": {},
   "source": [
    "# Send Trades To QuestDB directly\n",
    "\n",
    "This notebook will read the `./tradesMarch.csv` file to read trading events, and will send the events directly to QuestDB using multiple process in parallel.\n",
    "\n",
    "We first create the QuestDB table. It would automatically be created if it didn't exist in any case, but this way we can see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf9613c-bf7d-47be-8235-dab0de85e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore deprecation warnings in this demo\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9835b8-2b20-4b1c-b893-5ef647db99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg as pg\n",
    "import os\n",
    "\n",
    "# Fetch environment variables with defaults\n",
    "host = os.getenv('QDB_CLIENT_HOST', 'questdb')\n",
    "port = os.getenv('QDB_CLIENT_PORT', '8812')\n",
    "user = os.getenv('QDB_CLIENT_USER', 'admin')\n",
    "password = os.getenv('QDB_CLIENT_PASSWORD', 'quest')\n",
    "\n",
    "# Create the connection string using the environment variables or defaults\n",
    "conn_str = f'user={user} password={password} host={host} port={port} dbname=qdb'\n",
    "\n",
    "with pg.connect(conn_str, autocommit=True) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS  'trades' (\n",
    "  symbol SYMBOL capacity 256 CACHE,\n",
    "  side SYMBOL capacity 256 CACHE,\n",
    "  price DOUBLE,\n",
    "  amount DOUBLE,\n",
    "  timestamp TIMESTAMP\n",
    ") timestamp (timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol, side);\n",
    "\"\"\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416e215-ffd0-4aef-b526-a65143e7e554",
   "metadata": {},
   "source": [
    "## Sending the data to QuestDB\n",
    "\n",
    "Now we read the `./tradesMarch.csv` file and we insert into the trades table.\n",
    "\n",
    "By default, the script will override the original date with the current date and\n",
    " will wait 50ms between events before sending to QuestDB, to simulate a real time stream and provide\n",
    "a nicer visualization. You can override those configurations by changing the constants in the script. \n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the `TOTAL_EVENTS` number is reached. If the number of events on the CSV is smaller than the total events configured, the script will sumply loop over the file again.\n",
    "\n",
    "While the script is running, you can check the data in the table directly at QuestDB's web console at http://localhost:9000 or a live Grafana Dashboard powered by QuestDB at http://localhost:3000/d/trades-crypto-currency/trades-crypto-currency?orgId=1&refresh=250ms (user admin and password quest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7e3177-5269-433a-82af-5618181e90d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion started. Connecting to host.docker.internal:9000\n",
      "Sender 5 will send 2857143 events\n",
      "Sender 0 will send 2857143 events\n",
      "Sender 4 will send 2857143 events\n",
      "Sender 6 will send 2857142 events\n",
      "Sender 3 will send 2857143 events\n",
      "Sender 1 will send 2857143 events\n",
      "Sender 2 will send 2857143 events\n",
      "Sender 2 finished sending 2857143 events\n",
      "Sender 1 finished sending 2857143 events\n",
      "Sender 6 finished sending 2857142 events\n",
      "Sender 0 finished sending 2857143 events\n",
      "Sender 3 finished sending 2857143 events\n",
      "Sender 4 finished sending 2857143 events\n",
      "Sender 5 finished sending 2857143 events\n"
     ]
    }
   ],
   "source": [
    "from questdb.ingress import Sender, IngressError, TimestampNanos, ServerTimestamp\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "HTTP_ENDPOINT = os.getenv('QUESTDB_HTTP_ENDPOINT', 'questdb:9000')\n",
    "REST_TOKEN = os.getenv('QUESTDB_REST_TOKEN')\n",
    "\n",
    "TOTAL_EVENTS = 20_000_000  # Total events across all senders\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "NUM_SENDERS = 7  # Number of senders to execute in parallel\n",
    "CSV_FILE = './tradesMarch.csv'  # Path to the CSV file\n",
    "TIMESTAMP_FROM_FILE = False  # Whether to use the timestamp from the CSV file\n",
    "\n",
    "def send(sender_id, total_events, delay_ms=DELAY_MS, csv_file=CSV_FILE, http_endpoint=HTTP_ENDPOINT, auth=REST_TOKEN):\n",
    "    sys.stdout.write(f\"Sender {sender_id} will send {total_events} events\\n\")\n",
    "\n",
    "    try:\n",
    "        if auth is not None:\n",
    "            conf = f'https::addr={http_endpoint};tls_verify=unsafe_off;token={auth};'\n",
    "        else:\n",
    "            conf = f'http::addr={http_endpoint};'\n",
    "            \n",
    "        with Sender.from_conf(conf) as sender, open(csv_file, mode='r') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            events_sent = 0\n",
    "            csv_rows = list(csv_reader)  # Load the CSV data once into memory for looping\n",
    "\n",
    "            while events_sent < total_events:\n",
    "                row = csv_rows[events_sent % len(csv_rows)]  # Loop over the CSV rows\n",
    "\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_nanos = TimestampNanos(int(timestamp_dt.timestamp() * 1e9))  # Convert to nanoseconds\n",
    "                else:\n",
    "                    #timestamp_nanos = TimestampNanos.now()  # Get current time in nanoseconds\n",
    "                    timestamp_nanos = ServerTimestamp\n",
    "                \n",
    "                # Ingest the row with the current timestamp\n",
    "                sender.row(\n",
    "                    'trades',\n",
    "                    symbols={'symbol': row['symbol'], 'side': row['side']},\n",
    "                    columns={\n",
    "                        'price': float(row['price']),\n",
    "                        'amount': float(row['amount']),\n",
    "                    },\n",
    "                    at=timestamp_nanos  # Send timestamp in nanoseconds\n",
    "                )\n",
    "\n",
    "                events_sent += 1\n",
    "\n",
    "                # Delay after each event\n",
    "                if delay_ms > 0:\n",
    "                    time.sleep(delay_ms / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "            sys.stdout.write(f\"Sender {sender_id} finished sending {events_sent} events\\n\")\n",
    "\n",
    "    except IngressError as e:\n",
    "        sys.stderr.write(f'Sender {sender_id} got error: {e}\\n')\n",
    "\n",
    "def parallel_send(total_events, num_senders: int):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):  # Distribute the remaining events\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    with Pool(processes=num_senders) as pool:\n",
    "        sender_ids = range(num_senders)\n",
    "        pool.starmap(send, [(sender_id, sender_events[sender_id]) for sender_id in sender_ids])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stdout.write(f'Ingestion started. Connecting to {HTTP_ENDPOINT}\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713e0c-97dd-417d-9d44-b4d28231b459",
   "metadata": {},
   "source": [
    "## Verify we have ingested some data\n",
    "\n",
    "The data you send to Kafka will be processed by Kafka Connect and passed to QuestDB, where it will be stored into a table named `trades`. Let's check we can actually see some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5dafaac-9a8e-4c55-a7cb-3cb1453f9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:54:19.284340Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:54:19.285580Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:45.865026Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:46.183816Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:46.360160Z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.31.42.41'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "HTTP_ENDPOINT = os.getenv('QUESTDB_HTTP_ENDPOINT', 'questdb:9000')\n",
    "REST_TOKEN = os.getenv('QUESTDB_REST_TOKEN')\n",
    "\n",
    "if REST_TOKEN is not None:\n",
    "  host = f'https://admin:quest@{HTTP_ENDPOINT}'\n",
    "else:\n",
    "  host = f'http://admin:quest@questdb:9000'\n",
    "\n",
    "sql_query = 'SELECT * FROM trades LIMIT -5;'\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        host + '/exec',\n",
    "        params={'query': sql_query}, verify=False).json()\n",
    "    for row in response['dataset']:\n",
    "        print(row)    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
